# -*- coding: utf-8 -*-
"""final_TC3_22_2_23.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-F0xS_nnzBx7zK-CmVdCK11BKrO15UGR
"""

from sklearn.model_selection import GridSearchCV, train_test_split
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error
import pandas as pd
import numpy as np

df = pd.read_csv('/content/NEW_TC_UP.csv')

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df.drop(['TC'], axis=1), df['TC'], test_size=0.1, random_state=42)

param_grid = {
    'learning_rate': [0.1, 0.01, 0.5],
    'n_estimators': [100, 500, 900],
    'max_depth': [3, 5, 6],
    'subsample': [0.5, 0.7, 1.0],
    'colsample_bytree': [0.3, 0.4, 0.6],
    'gamma': [0, 1, 4],
}

# Create the XGBoost regressor object
xgb = XGBRegressor()

# Create the GridSearchCV object
grid_search = GridSearchCV(xgb, param_grid=param_grid, cv=10, n_jobs=-1)

# Fit the GridSearchCV object to the data
grid_search.fit(X_train, y_train)

# Print the best hyperparameters and the corresponding score
print("Best parameters:", grid_search.best_params_)
print("Best score:", grid_search.best_score_)

# Define the XGBoost regressor with the best hyperparameters
xgb = XGBRegressor(colsample_bytree=grid_search.best_params_['colsample_bytree'],
                   gamma=grid_search.best_params_['gamma'],
                   learning_rate=grid_search.best_params_['learning_rate'],
                   max_depth=grid_search.best_params_['max_depth'],
                   n_estimators=grid_search.best_params_['n_estimators'],
                   subsample=grid_search.best_params_['subsample'])

# Train and run  the model on the training data
xgb.fit(X_train, y_train)

# Make predictions Train and test TC data
y_train_pred = xgb.predict(X_train)
y_test_pred = xgb.predict(X_test)

# Compute the RMSE, MAE, and MSE for the training
train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
train_mae = mean_absolute_error(y_train, y_train_pred)
train_mse = mean_squared_error(y_train, y_train_pred)

# Print the above metrices for the training and testing results
print("Training RMSE:", train_rmse)
print("Training MAE:", train_mae)
print("Training MSE:", train_mse)

import matplotlib.pyplot as plt
import seaborn as sns

# Make predictions on the test and train set
y_pred_train = xgb.predict(X_train)
y_pred = xgb.predict(X_test)

# Set up the plot
fig = plt.gcf()
fig.set_size_inches(8,8)
a = plt.axes(aspect='equal')

# Plot the predicted vs true values for train and test set
plt.scatter(y_train, y_pred_train, marker="o", label="train", s = 100, facecolors='aqua', edgecolors='black', linewidth = 1.7)
plt.scatter(y_test, y_pred, marker="s", label="test", s = 100, facecolors='red', edgecolors='indigo', linewidth = 1.7)

# Set axis labels and limits
plt.xlabel('Experimental TC', fontsize=25, fontweight='bold')
plt.ylabel('ML Predicted TC', fontsize=25, fontweight='bold')
plt.xticks(fontsize=20, fontweight='bold')
plt.yticks(fontsize=20, fontweight='bold')
plt.xlim([-50, 480])
plt.ylim([-50, 480])

# Add legend and diagonal line
plt.legend(loc='upper left', fontsize=35)
plt.plot([-50, 480], [-50, 480], 'b', lw=3)

plt.tight_layout()
plt.show()

# Load the new test data (with only independent variables)
new_test_data = pd.read_csv("/content/new_ICONEL.csv")

# Make predictions on the new test data
y_pred = xgb.predict(new_test_data)

print("new_test_data",y_pred)

