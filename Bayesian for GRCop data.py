# -*- coding: utf-8 -*-
"""BHT-GRCop-42.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13aTnpkZs5_AwL_N1qv_IH3h0PKJt9-x3
"""

#!pip install bayesian-optimization

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from bayes_opt import BayesianOptimization

df = pd.read_csv('/content/New_input_GRCOP.csv')

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df.drop(['TC'], axis=1), df['TC'], test_size=0.1, random_state=42)

# Define the XGBRegressor model
xgb = XGBRegressor()

# Define the parameter space for hyperparameter tuning using Bayesian optimization
pbounds = {'max_depth': (1,6),
           'learning_rate': (0.01, 0.4),
           'n_estimators': (100, 700),
           'subsample': (0.1, 1),
           'colsample_bytree': (0.1, 1),
           'min_child_weight': (1, 4),
           'gamma': (0, 4)}

def optimize_xgb(max_depth, learning_rate, n_estimators, subsample, colsample_bytree, min_child_weight, gamma):
    xgb.set_params(max_depth=int(max_depth),
                   learning_rate=learning_rate,
                   n_estimators=int(n_estimators),
                   subsample=subsample,
                   colsample_bytree=colsample_bytree,
                   min_child_weight=int(min_child_weight),
                   gamma=gamma)
    
    # Fit the XGBRegressor model on the training dataset
    xgb.fit(X_train, y_train)

    # Predict the target variable on the testing dataset
    y_pred = xgb.predict(X_test)

    # Calculate and return the negative RMSE
    return -np.sqrt(mean_squared_error(y_test, y_pred))

# Perform Bayesian optimization
optimizer = BayesianOptimization(
    f=optimize_xgb,
    pbounds=pbounds,
    random_state=1,
    verbose=2
)
optimizer.maximize(init_points=5, n_iter=600)

# Print the best parameter values
print(optimizer.max)

# Get the best hyperparameters
params = optimizer.max['params']
params['max_depth'] = int(params['max_depth'])
params['n_estimators'] = int(params['n_estimators'])
params['min_child_weight'] = int(params['min_child_weight'])

# Re-fit the XGBRegressor model on the training dataset with the best hyperparameters
xgb = XGBRegressor(**params)
xgb.fit(X_train, y_train)

# Predict the target variable on the testing dataset
y_pred = xgb.predict(X_test)

# Evaluate the performance of the model on the testing dataset
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# Print the performance metrics
print("RMSE: ", rmse)
print("MSE: ", mse)
print("MAE: ", mae)
print("R2: ", r2)

# Load the new test data (with only independent variables)
new_test_data = pd.read_csv("/content/Grcop_test_data_M30.csv")

# Make predictions on the new test data
y_pred = xgb.predict(new_test_data)

print("new_test_data",y_pred)

#save the predictions to a file
new_test_data["predicted_target_variable"] = y_pred
new_test_data.to_csv("predicted_test_data_XGB_model.csv", index=False)